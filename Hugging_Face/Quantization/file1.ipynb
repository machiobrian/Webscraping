{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long short term memory\n",
    "# a special recurrent neural net\n",
    "# brings persistence in inferencing. we do not start from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantization allows one make a trade-off btn performance and accuracy with a known models after\n",
    "# its training is complete\n",
    "\n",
    "# Quantization -> instantiate a floating point model and create a quantized version of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the modules to be used in the recpe\n",
    "\n",
    "import torch\n",
    "import torch.quantization\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_for_demo(nn.Module):\n",
    "    \"\"\"\n",
    "        Elementary long short term memory style.\n",
    "        Wraps up an nn.LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, depth):\n",
    "        super(lstm_for_demo, self).__init__()\n",
    "        self.lstm = nn.LSTM(in_dim, out_dim, depth)\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        out, hidden = self.lstm(inputs, hidden)\n",
    "        return out, hidden\n",
    "\n",
    "torch.manual_seed(29592) #seeds for reproducibility\n",
    "\n",
    "#shape params\n",
    "model_dimension=8\n",
    "sequence_length = 20\n",
    "batch_size = 1\n",
    "lstm_depth = 1\n",
    "\n",
    "#random data for input\n",
    "inputs = torch.randn(sequence_length, batch_size, model_dimension)\n",
    "#hidden is actually a tuple of the inintial hidden state and the initial cell state\n",
    "hidden = (torch.randn(lstm_depth, batch_size, model_dimension), \n",
    "torch.randn(lstm_depth, batch_size, model_dimension))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a floating point instance\n",
    "float_lstm = lstm_for_demo(model_dimension, model_dimension, lstm_depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dc128987b80414afb03073ef0ac1a5f16932121e6908c8d5ab86fd86be48a1ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
