{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b5421bdbebd40de9c1cbb5856445210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets retrieve time steps for a model\n",
    "\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModelForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import datasets\n",
    "\n",
    "# import mddel, featureextractot, tokenizer\n",
    "model = AutoModelForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3874d113d93f4bbc91df93a8eab3085b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/26.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d466601f80514fab9e55b2558f14347d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/174k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5092d11a6954470b9df039688c521f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mcast_column(\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m, datasets\u001b[39m.\u001b[39mAudio(sampling_rate\u001b[39m=\u001b[39m\u001b[39m16_000\u001b[39m))\n\u001b[1;32m      4\u001b[0m dataset_iter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(dataset)\n\u001b[0;32m----> 5\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataset_iter)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/datasets/iterable_dataset.py:782\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 782\u001b[0m     \u001b[39mfor\u001b[39;00m key, example \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter():\n\u001b[1;32m    783\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures:\n\u001b[1;32m    784\u001b[0m             \u001b[39m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m    785\u001b[0m             \u001b[39m# This is done with `_apply_feature_types`.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m             \u001b[39myield\u001b[39;00m _apply_feature_types(example, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, token_per_repo_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_per_repo_id)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/datasets/iterable_dataset.py:772\u001b[0m, in \u001b[0;36mIterableDataset._iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     ex_iterable \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ex_iterable\n\u001b[0;32m--> 772\u001b[0m \u001b[39myield from\u001b[39;00m ex_iterable\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/datasets/iterable_dataset.py:142\u001b[0m, in \u001b[0;36mExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_examples_fn(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/common_voice/a1dc74461f6c839bfe1e8cf1262fd4cf24297e3fbd4087a711bd090779023a5e/common_voice.py:767\u001b[0m, in \u001b[0;36mCommonVoice._generate_examples\u001b[0;34m(self, local_extracted_archive, archive_iterator, metadata_filepath, path_to_clips)\u001b[0m\n\u001b[1;32m    765\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39m==\u001b[39m metadata_filepath:\n\u001b[1;32m    766\u001b[0m     metadata_found \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m     lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39;49mreadlines()\n\u001b[1;32m    768\u001b[0m     headline \u001b[39m=\u001b[39m lines[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    770\u001b[0m     column_names \u001b[39m=\u001b[39m headline\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/tarfile.py:696\u001b[0m, in \u001b[0;36m_FileInFile.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreadinto\u001b[39m(\u001b[39mself\u001b[39m, b):\n\u001b[0;32m--> 696\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m(b))\n\u001b[1;32m    697\u001b[0m     b[:\u001b[39mlen\u001b[39m(buf)] \u001b[39m=\u001b[39m buf\n\u001b[1;32m    698\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(buf)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/tarfile.py:685\u001b[0m, in \u001b[0;36m_FileInFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    684\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfileobj\u001b[39m.\u001b[39mseek(offset \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition \u001b[39m-\u001b[39m start))\n\u001b[0;32m--> 685\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfileobj\u001b[39m.\u001b[39;49mread(length)\n\u001b[1;32m    686\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(b) \u001b[39m!=\u001b[39m length:\n\u001b[1;32m    687\u001b[0m         \u001b[39mraise\u001b[39;00m ReadError(\u001b[39m\"\u001b[39m\u001b[39munexpected end of data\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/tarfile.py:522\u001b[0m, in \u001b[0;36m_Stream.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[39m\"\"\"Return the next size number of bytes from the stream.\"\"\"\u001b[39;00m\n\u001b[1;32m    521\u001b[0m \u001b[39massert\u001b[39;00m size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 522\u001b[0m buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read(size)\n\u001b[1;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(buf)\n\u001b[1;32m    524\u001b[0m \u001b[39mreturn\u001b[39;00m buf\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/tarfile.py:540\u001b[0m, in \u001b[0;36m_Stream._read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    539\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 540\u001b[0m     buf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfileobj\u001b[39m.\u001b[39;49mread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbufsize)\n\u001b[1;32m    541\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m buf:\n\u001b[1;32m    542\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:357\u001b[0m, in \u001b[0;36m_add_retries_to_file_obj_read_method.<locals>.read_with_retries\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[39mfor\u001b[39;00m retry \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, max_retries \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[1;32m    356\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 357\u001b[0m         out \u001b[39m=\u001b[39m read(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    358\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    359\u001b[0m     \u001b[39mexcept\u001b[39;00m (ClientError, \u001b[39mTimeoutError\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/fsspec/implementations/http.py:574\u001b[0m, in \u001b[0;36mHTTPFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    573\u001b[0m     length \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc, length)\n\u001b[0;32m--> 574\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mread(length)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/fsspec/spec.py:1575\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1572\u001b[0m \u001b[39mif\u001b[39;00m length \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1573\u001b[0m     \u001b[39m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1574\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1575\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache\u001b[39m.\u001b[39;49m_fetch(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc \u001b[39m+\u001b[39;49m length)\n\u001b[1;32m   1576\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(out)\n\u001b[1;32m   1577\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/fsspec/caching.py:394\u001b[0m, in \u001b[0;36mBytesCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m=\u001b[39m start\n\u001b[1;32m    393\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m         new \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfetcher(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mend, bend)\n\u001b[1;32m    395\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache \u001b[39m+\u001b[39m new\n\u001b[1;32m    397\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mend \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart \u001b[39m+\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/fsspec/asyn.py:111\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    109\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    110\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m obj \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 111\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/site-packages/fsspec/asyn.py:84\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     82\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     83\u001b[0m     \u001b[39m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mif\u001b[39;00m event\u001b[39m.\u001b[39;49mwait(\u001b[39m1\u001b[39;49m):\n\u001b[1;32m     85\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#load first sample of English common-voice\n",
    "dataset = load_dataset(\"common_voice\",\"en\",split=\"train\", streaming=True)\n",
    "dataset = dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=16_000))\n",
    "dataset_iter = iter(dataset)\n",
    "sample = next(dataset_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward sample through model to get greedily predicted transcripts ids\n",
    "input_values = feature_extractor(sample[\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "logits = model(input_values).logits[0]\n",
    "pred_ids = torch.argmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve word stamps (analog commands for output_char_offsets)\n",
    "outputs = tokenizer.decode(pred_ids, output_word_offset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute time_offset in seconds as a product of downsampling ration and sampling rate\n",
    "time_offsest = model.config.inputs_to_logitd_ratio/ feature_extractor.sampling_rate\n",
    "\n",
    "eord_offsets = [\n",
    "    {\n",
    "        \"word\":d[\"word\"],\n",
    "        \"start_time\":round(d[\"start_offset\"]*time_offsest,2)\n",
    "        \"end_time\": round(d[\"end_offset\"]*time_offsest,2)\n",
    "    }\n",
    "    for d in outputs.word_offsets\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare word offsets with audio common_voice_en_100038.mp3 online on the dataset viewer:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "17c74939301d4ce31ce75955178464d5e4340f201adfe28f9541d514feeca86b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
